{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DePSI: Delft PS-InSAR processing package","text":"<p>DePSI (van Leijen, 2014) is an open source software for processing Persistent Scatterer Interferometry (PS-InSAR) data, originally implemented in MATLAB. From 2024, TUDelft and Netherlands eScience Center are collaborating to develop a Python version of DePSI, with recent advances in PS-InSAR. </p> <p>For the stable version of DePSI implemented in MATLAB, please refer to the stable branch.</p>"},{"location":"#references","title":"References","text":"<p>[1] Van Leijen, Frederik Johannes. \"Persistent scatterer interferometry based on geodetic estimation theory.\" (2014).</p>"},{"location":"api_reference/","title":"API Reference","text":""},{"location":"api_reference/#classification-methods","title":"Classification methods:","text":"<p>Functions for scatterer selection related operations.</p>"},{"location":"api_reference/#depsi.classification.network_stm_selection","title":"<code>network_stm_selection(stm, min_dist, include_index=None, sortby_var='pnt_nmad', crs='radar', x_var='azimuth', y_var='range', azimuth_spacing=None, range_spacing=None)</code>","text":"<p>Select a Space-Time Matrix (STM) from a candidate STM for network processing.</p> <p>The selection is based on two criteria: 1. A minimum distance between selected points. 2. A sorting metric to select better points.</p> <p>The candidate STM will be sorted by the sorting metric. The selection will be performed iteratively, starting from the best point. In each iteration, the best point will be selected, and points within the minimum distance will be removed. The process will continue until no points are left in the candidate STM.</p> <p>Parameters:</p> Name Type Description Default <code>stm</code> <code>Dataset</code> <p>candidate Space-Time Matrix (STM).</p> required <code>min_dist</code> <code>int | float</code> <p>Minimum distance between selected points.</p> required <code>include_index</code> <code>list[int]</code> <p>Index of points in the candidate STM that must be included in the selection, by default None</p> <code>None</code> <code>sortby_var</code> <code>str</code> <p>Sorting metric for selecting points, by default \"pnt_nmad\"</p> <code>'pnt_nmad'</code> <code>crs</code> <code>int | str</code> <p>EPSG code of Coordinate Reference System of <code>x_var</code> and <code>y_var</code>, by default \"radar\". If crs is \"radar\", the distance will be calculated based on radar coordinates, and azimuth_spacing and range_spacing must be provided.</p> <code>'radar'</code> <code>x_var</code> <code>str</code> <p>Data variable name for x coordinate, by default \"azimuth\"</p> <code>'azimuth'</code> <code>y_var</code> <code>str</code> <p>Data variable name for y coordinate, by default \"range\"</p> <code>'range'</code> <code>azimuth_spacing</code> <code>float</code> <p>Azimuth spacing, by default None. Required if crs is \"radar\".</p> <code>None</code> <code>range_spacing</code> <code>float</code> <p>Range spacing, by default None. Required if crs is \"radar\".</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Selected network Space-Time Matrix (STM).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raised when <code>azimuth_spacing</code> or <code>range_spacing</code> is not provided for radar coordinates.</p> <code>NotImplementedError</code> <p>Raised when an unsupported Coordinate Reference System is provided.</p> Source code in <code>depsi/classification.py</code> <pre><code>def network_stm_selection(\n    stm: xr.Dataset,\n    min_dist: int | float,\n    include_index: list[int] = None,\n    sortby_var: str = \"pnt_nmad\",\n    crs: int | str = \"radar\",\n    x_var: str = \"azimuth\",\n    y_var: str = \"range\",\n    azimuth_spacing: float = None,\n    range_spacing: float = None,\n):\n    \"\"\"Select a Space-Time Matrix (STM) from a candidate STM for network processing.\n\n    The selection is based on two criteria:\n    1. A minimum distance between selected points.\n    2. A sorting metric to select better points.\n\n    The candidate STM will be sorted by the sorting metric.\n    The selection will be performed iteratively, starting from the best point.\n    In each iteration, the best point will be selected, and points within the minimum distance will be removed.\n    The process will continue until no points are left in the candidate STM.\n\n    Parameters\n    ----------\n    stm : xr.Dataset\n        candidate Space-Time Matrix (STM).\n    min_dist : int | float\n        Minimum distance between selected points.\n    include_index : list[int], optional\n        Index of points in the candidate STM that must be included in the selection, by default None\n    sortby_var : str, optional\n        Sorting metric for selecting points, by default \"pnt_nmad\"\n    crs : int | str, optional\n        EPSG code of Coordinate Reference System of `x_var` and `y_var`, by default \"radar\".\n        If crs is \"radar\", the distance will be calculated based on radar coordinates, and\n        azimuth_spacing and range_spacing must be provided.\n    x_var : str, optional\n        Data variable name for x coordinate, by default \"azimuth\"\n    y_var : str, optional\n        Data variable name for y coordinate, by default \"range\"\n    azimuth_spacing : float, optional\n        Azimuth spacing, by default None. Required if crs is \"radar\".\n    range_spacing : float, optional\n        Range spacing, by default None. Required if crs is \"radar\".\n\n    Returns\n    -------\n    xr.Dataset\n        Selected network Space-Time Matrix (STM).\n\n    Raises\n    ------\n    ValueError\n        Raised when `azimuth_spacing` or `range_spacing` is not provided for radar coordinates.\n    NotImplementedError\n        Raised when an unsupported Coordinate Reference System is provided.\n    \"\"\"\n    match crs:\n        case \"radar\":\n            if (azimuth_spacing is None) or (range_spacing is None):\n                raise ValueError(\"Azimuth and range spacing must be provided for radar coordinates.\")\n        case _:\n            raise NotImplementedError\n\n    # Get coordinates and sorting metric, load them into memory\n    stm_select = None\n    stm_remain = stm[[x_var, y_var, sortby_var]].compute()\n\n    # Select the include_index if provided\n    if include_index is not None:\n        stm_select = stm_remain.isel(space=include_index)\n\n        # Remove points within min_dist of the included points\n        coords_include = np.column_stack(\n            [stm_select[\"azimuth\"].values * azimuth_spacing, stm_select[\"range\"].values * range_spacing]\n        )\n        coords_remain = np.column_stack(\n            [stm_remain[\"azimuth\"].values * azimuth_spacing, stm_remain[\"range\"].values * range_spacing]\n        )\n        idx_drop = _idx_within_distance(coords_include, coords_remain, min_dist)\n        if idx_drop is not None:\n            stm_remain = stm_remain.where(~(stm_remain[\"space\"].isin(idx_drop)), drop=True)\n\n    # Reorder the remaining points by the sorting metric\n    stm_remain = stm_remain.sortby(sortby_var)\n\n    # Build a list of the index of selected points\n    if stm_select is None:\n        space_idx_sel = []\n    else:\n        space_idx_sel = stm_select[\"space\"].values.tolist()\n\n    while stm_remain.sizes[\"space\"] &gt; 0:\n        # Select one point with best sorting metric\n        stm_now = stm_remain.isel(space=0)\n\n        # Append the selected point index\n        space_idx_sel.append(stm_now[\"space\"].values.tolist())\n\n        # Remove the selected point from the remaining points\n        stm_remain = stm_remain.isel(space=slice(1, None)).copy()\n\n        # Remove points in stm_remain within min_dist of stm_now\n        coords_remain = np.column_stack(\n            [stm_remain[\"azimuth\"].values * azimuth_spacing, stm_remain[\"range\"].values * range_spacing]\n        )\n        coords_stmnow = np.column_stack(\n            [stm_now[\"azimuth\"].values * azimuth_spacing, stm_now[\"range\"].values * range_spacing]\n        )\n        idx_drop = _idx_within_distance(coords_stmnow, coords_remain, min_dist)\n        if idx_drop is not None:\n            stm_drop = stm_remain.isel(space=idx_drop)\n            stm_remain = stm_remain.where(~(stm_remain[\"space\"].isin(stm_drop[\"space\"])), drop=True)\n\n    # Get the selected points by space index from the original stm\n    stm_out = stm.sel(space=space_idx_sel)\n\n    return stm_out\n</code></pre>"},{"location":"api_reference/#depsi.classification.ps_selection","title":"<code>ps_selection(slcs, threshold, method='nad', output_chunks=10000, mem_persist=False)</code>","text":"<p>Select Persistent Scatterers (PS) from an SLC stack, and return a Space-Time Matrix.</p> <p>The selection method is defined by <code>method</code> and <code>threshold</code>. The selected pixels will be reshaped to (space, time), where <code>space</code> is the number of selected pixels. The unselected pixels will be discarded. The original <code>azimuth</code> and <code>range</code> coordinates will be persisted. The computed NAD or NMAD will be added to the output dataset as a new variable. It can be persisted in memory if <code>mem_persist</code> is True.</p> <p>Parameters:</p> Name Type Description Default <code>slcs</code> <code>Dataset</code> <p>Input SLC stack. It should have the following dimensions: (\"azimuth\", \"range\", \"time\"). There should be a <code>amplitude</code> variable in the dataset.</p> required <code>threshold</code> <code>float</code> <p>Threshold value for selection.</p> required <code>method</code> <code>Literal['nad', 'nmad']</code> <p>Method of selection, by default \"nad\". - \"nad\": Normalized Amplitude Dispersion - \"nmad\": Normalized median absolute deviation</p> <code>'nad'</code> <code>output_chunks</code> <code>int</code> <p>Chunk size in the <code>space</code> dimension, by default 10000</p> <code>10000</code> <code>mem_persist</code> <code>bool</code> <p>If true persist the NAD or NMAD in memory, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Selected STM, in form of an xarray.Dataset with two dimensions: (space, time).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Raised when an unsupported method is provided.</p> Source code in <code>depsi/classification.py</code> <pre><code>def ps_selection(\n    slcs: xr.Dataset,\n    threshold: float,\n    method: Literal[\"nad\", \"nmad\"] = \"nad\",\n    output_chunks: int = 10000,\n    mem_persist: bool = False,\n) -&gt; xr.Dataset:\n    \"\"\"Select Persistent Scatterers (PS) from an SLC stack, and return a Space-Time Matrix.\n\n    The selection method is defined by `method` and `threshold`.\n    The selected pixels will be reshaped to (space, time), where `space` is the number of selected pixels.\n    The unselected pixels will be discarded.\n    The original `azimuth` and `range` coordinates will be persisted.\n    The computed NAD or NMAD will be added to the output dataset as a new variable. It can be persisted in\n    memory if `mem_persist` is True.\n\n    Parameters\n    ----------\n    slcs : xr.Dataset\n        Input SLC stack. It should have the following dimensions: (\"azimuth\", \"range\", \"time\").\n        There should be a `amplitude` variable in the dataset.\n    threshold : float\n        Threshold value for selection.\n    method : Literal[\"nad\", \"nmad\"], optional\n        Method of selection, by default \"nad\".\n        - \"nad\": Normalized Amplitude Dispersion\n        - \"nmad\": Normalized median absolute deviation\n    output_chunks : int, optional\n        Chunk size in the `space` dimension, by default 10000\n    mem_persist : bool, optional\n        If true persist the NAD or NMAD in memory, by default False.\n\n\n    Returns\n    -------\n    xr.Dataset\n        Selected STM, in form of an xarray.Dataset with two dimensions: (space, time).\n\n    Raises\n    ------\n    NotImplementedError\n        Raised when an unsupported method is provided.\n    \"\"\"\n    # Make sure there is no temporal chunk\n    # since later a block function assumes all temporal data is available in a spatial block\n    slcs = slcs.chunk({\"time\": -1})\n\n    # Calculate selection mask\n    match method:\n        case \"nad\":\n            nad = xr.map_blocks(\n                _nad_block, slcs[\"amplitude\"], template=slcs[\"amplitude\"].isel(time=0).drop_vars(\"time\")\n            )\n            nad = nad.compute() if mem_persist else nad\n            slcs = slcs.assign(pnt_nad=nad)\n            mask = nad &lt; threshold\n        case \"nmad\":\n            nmad = xr.map_blocks(\n                _nmad_block, slcs[\"amplitude\"], template=slcs[\"amplitude\"].isel(time=0).drop_vars(\"time\")\n            )\n            nmad = nmad.compute() if mem_persist else nmad\n            slcs = slcs.assign(pnt_nmad=nmad)\n            mask = nmad &lt; threshold\n        case _:\n            raise NotImplementedError\n\n    # Get the 1D index on space dimension\n    mask_1d = mask.stack(space=(\"azimuth\", \"range\")).drop_vars([\"azimuth\", \"range\", \"space\"])  # Drop multi-index coords\n    index = mask_1d[\"space\"].where(mask_1d.compute(), other=0, drop=True)  # Evaluate the 1D mask to index\n\n    # Reshape from Stack (\"azimuth\", \"range\", \"time\") to Space-Time Matrix  (\"space\", \"time\")\n    stacked = slcs.stack(space=(\"azimuth\", \"range\"))\n\n    # Drop multi-index coords for space coordinates\n    # This will also azimuth and range coordinates, as they are part of the multi-index coordinates\n    stm = stacked.drop_vars([\"space\", \"azimuth\", \"range\"])\n\n    # Assign a continuous index the space dimension\n    # Assign azimuth and range back as coordinates\n    stm = stm.assign_coords(\n        {\n            \"space\": ([\"space\"], range(stm.sizes[\"space\"])),\n            \"azimuth\": ([\"space\"], stacked[\"azimuth\"].values),\n            \"range\": ([\"space\"], stacked[\"range\"].values),\n        }\n    )  # keep azimuth and range as coordinates\n\n    # Apply selection\n    stm_masked = stm.sel(space=index)\n\n    # Re-order the dimensions to community preferred (\"space\", \"time\") order\n    stm_masked = stm_masked.transpose(\"space\", \"time\")\n\n    # Rechunk is needed because after apply maksing, the chunksize will be inconsistant\n    stm_masked = stm_masked.chunk(\n        {\n            \"space\": output_chunks,\n            \"time\": -1,\n        }\n    )\n\n    # Reset space coordinates\n    stm_masked = stm_masked.assign_coords(\n        {\n            \"space\": ([\"space\"], range(stm_masked.sizes[\"space\"])),\n        }\n    )\n\n    # Compute NAD or NMAD if mem_persist is True\n    # This only evaluate a very short task graph, since NAD or NMAD is already in memory\n    if mem_persist:\n        match method:\n            case \"nad\":\n                stm_masked[\"pnt_nad\"] = stm_masked[\"pnt_nad\"].compute()\n            case \"nmad\":\n                stm_masked[\"pnt_nmad\"] = stm_masked[\"pnt_nmad\"].compute()\n\n    return stm_masked\n</code></pre>"},{"location":"api_reference/#io-methods","title":"IO methods:","text":""},{"location":"api_reference/#depsi.io","title":"<code>depsi.io</code>","text":"<p>io methods.</p>"},{"location":"api_reference/#depsi.io.read_metadata","title":"<code>read_metadata(resfile, mode='raw', **kwargs)</code>","text":"<p>Read metadata from a DORIS v5 resfile.</p> <p>Modified from the original functions in: https://github.com/Pbaz98/Caroline-Radar-Coding-Toolbox/blob/main/gecoris/dorisUtils.py.</p> Source code in <code>depsi/io.py</code> <pre><code>def read_metadata(resfile, mode=\"raw\", **kwargs):\n    \"\"\"Read metadata from a DORIS v5 resfile.\n\n    Modified from the original functions in:\n    https://github.com/Pbaz98/Caroline-Radar-Coding-Toolbox/blob/main/gecoris/dorisUtils.py.\n    \"\"\"\n    # check crop_flag\n    if mode == \"coreg\" and \"crop\" in kwargs:\n        crop_flag = kwargs[\"crop\"]\n    else:\n        crop_flag = 0\n\n    # Open the file\n    with open(resfile) as file:\n        content = file.read()\n\n    # +++   - Satellite ID\n    pattern = r\"Product type specifier:\\s+(.*?(?=\\n))\"\n    match = re.search(pattern, content)\n    sat_id = match.group(1).upper()\n\n    # ++++1 - Geometry [DESCENDING or ASCENDING]\n    pattern = r\"PASS:\\s+(.*?(?=\\n))\"\n    match = re.search(pattern, content)\n    geometry = match.group(1).upper()\n\n    # ++++ 2 - Acquisition Date [dictionary with __datetime__ and str, in the format 'yyyy-mm-dd hh:mm:ss']\n    pattern = r\"First_pixel_azimuth_time \\(UTC\\):\\s+(\\d+-\\w+-\\d+\\s+)(\\d+:\\d+:\\d+)\"\n    match = re.search(pattern, content)\n\n    # --- extract the datetime string\n    datetime_toconvert = match.group(1) + match.group(2)\n    # Parse the original datetime string\n    acq_date = datetime.strptime(datetime_toconvert, \"%Y-%b-%d %H:%M:%S\")\n    acq_date.strftime(\"%Y%m%d\")\n\n    # ++++ 3 - azimuth0time\n\n    # convert from time format to seconds of the day\n    pattern = r\"(\\d+):(\\d+):(\\d+.\\d+)\"\n    match = re.search(pattern, content)\n    azimuth0time = int(match.group(1)) * 3600 + int(match.group(2)) * 60 + float(match.group(3))\n\n    # ++++ 4 - range0time\n    pattern = r\"Range_time_to_first_pixel \\(2way\\) \\(ms\\):\" + SC_N_PATTERN\n    match = re.search(pattern, content)\n    range0time = float(match.group(1)) * 1e-3 / 2  # devide by 2 to balance the two way travel\n\n    # ++++ 5 - prf\n    pattern = r\"Pulse_Repetition_Frequency \\(computed, Hz\\):\" + SC_N_PATTERN\n    match = re.search(pattern, content)\n    prf = float(match.group(1))\n\n    # ++++ 6 - rsr\n    pattern = r\"Range_sampling_rate \\(computed, MHz\\):\" + SC_N_PATTERN\n    match = re.search(pattern, content)\n    rsr = float(match.group(1)) * 1e6 * 2\n\n    # ++++ 7 - wavelength\n    pattern = r\"Radar_wavelength \\(m\\):\" + SC_N_PATTERN\n    match = re.search(pattern, content)\n    wavelength = float(match.group(1))\n\n    # ++++ 8 - orbit_fit\n\n    # Define the regular expression pattern to match the table rows\n    pattern = r\"(\\d+)\\s+([-+]?\\d+\\.\\d+(?:\\.\\d+)?)\\s+([-+]?\\d+\\.\\d+(?:\\.\\d+)?)\\s+([-+]?\\d+\\.\\d+(?:\\.\\d+)?)\"\n\n    # extract the table rows\n    table_rows = re.findall(pattern, content)\n\n    orbit = np.ones((len(table_rows), 4))\n\n    for i in range(len(table_rows)):\n        for j in range(4):\n            orbit[i][j] = float(table_rows[i][j])\n\n    # Generate the orbfit dictionary\n    orbfit = _orbit_fit(orbit, verbose=0)\n\n    # ++++ 9 - range_spacing\n    pattern = r\"rangePixelSpacing:\" + SC_N_PATTERN\n    match = re.search(pattern, content)\n    range_spacing = float(match.group(1))\n\n    # ++++ 10 - azimuth_spacing\n    pattern = r\"azimuthPixelSpacing:\" + SC_N_PATTERN\n    match = re.search(pattern, content)\n    azimuth_spacing = float(match.group(1))\n\n    # ++++ 11 - center_lon\n    pattern = r\"Scene_centre_longitude:\" + SC_N_PATTERN\n    match = re.search(pattern, content)\n    center_lon = float(match.group(1))\n\n    # ++++ 12 - center_lat\n    pattern = r\"Scene_centre_latitude:\" + SC_N_PATTERN\n    match = re.search(pattern, content)\n    center_lat = float(match.group(1))\n\n    # ++++ 13 - center_h\n    pattern = r\"Scene_center_heading:\" + SC_N_PATTERN\n    match = re.search(pattern, content)\n    center_h = float(match.group(1))\n\n    # ++++ 14 - n_azimuth\n    pattern = r\"Number_of_lines_original:\" + SC_N_PATTERN\n    match = re.search(pattern, content)\n    n_azimuth = int(match.group(1))\n\n    # ++++ 15 - n_range\n    pattern = r\"Number_of_pixels_original:\" + SC_N_PATTERN\n    match = re.search(pattern, content)\n    n_range = int(match.group(1))\n\n    # ++++ 16 - swath\n    pattern = r\"SWATH:\\s+IW(\\d+)\"\n    match = re.search(pattern, content)\n    swath = int(match.group(1))\n\n    # ++++ 17 - center_azimuth\n    center_azimuth = np.round(n_azimuth / 2)\n\n    # ++++ 18 - beta0, rank, chirprate\n    beta0 = 237\n    if swath == 1:\n        rank = 9\n        chirp_rate = 1078230321255.894\n    elif swath == 2:\n        rank = 8\n        chirp_rate = 779281727512.0481\n    elif swath == 3:\n        rank = 10\n        chirp_rate = 801450949070.5804\n\n    # resolutions [from s1 annual performance reports]\n    az_resolutions = np.array([21.76, 21.89, 21.71])\n    sr_resolutions = np.array([2.63, 3.09, 3.51])  # slant range resolution\n    azimuth_resolution = az_resolutions[swath - 1]\n\n    # ++++ 20 - range_resolution\n    pattern = r\"Total_range_band_width \\(MHz\\):\" + SC_N_PATTERN\n    match = re.search(pattern, content)\n    range_resolution = SPEED_OF_LIGHT / (2 * float(match.group(1)) * 1e6)\n\n    # ++++ 21 - nBursts\n    burst_n = None\n\n    # ++++ 23 - steering_rate\n    pattern = r\"Azimuth_steering_rate \\(deg/s\\):\" + SC_N_PATTERN\n    match = re.search(pattern, content)\n    steering_rate = float(match.group(1)) * np.pi / 180\n\n    # ++++ 24 and 25 - azFmRateArray and dcPolyArray\n    # Are skipped because the io.datetimeToMJD function is missing\n\n    # ++++ 26 - pri\n    pattern = r\"Pulse_Repetition_Frequency_raw_data\\(TOPSAR\\):\" + SC_N_PATTERN\n    match = re.search(pattern, content)\n    pri = 1 / float(match.group(1))\n\n    # ++++ 27 - rank\n    # See Beta0 section\n\n    # ++++ 28 - chirp_rate\n\n    # ++++ 29 - n_azimuth\n    if crop_flag:\n        crop_file = \"/\".join(str(resfile).split(\"/\")[0:-2]) + \"/nlines_crp.txt\"\n        with open(crop_file) as file:\n            content = file.readlines()\n            n_lines, first_line, last_line = (\n                int(content[0].strip()),\n                int(content[1].strip()),\n                int(content[2].strip()),\n            )\n\n    else:\n        # Extract first\n        pattern = r\"First_line \\(w.r.t. original_image\\):\" + SC_N_PATTERN\n        match = re.search(pattern, content)\n        first_line = int(match.group(1))\n        # Extract last\n        pattern = r\"Last_line \\(w.r.t. original_image\\):\" + SC_N_PATTERN\n        match = re.search(pattern, content)\n        last_line = int(match.group(1))\n        # difference\n        n_lines = last_line - first_line + 1\n\n    # ++++ 30 - n_range\n    if crop_flag:\n        crop_file = \"/\".join(str(resfile).split(\"/\")[0:-2]) + \"/npixels_crp.txt\"\n        with open(crop_file) as file:\n            content = file.readlines()\n            n_pixels, first_pixel, last_pixel = (\n                int(content[0].strip()),\n                int(content[1].strip()),\n                int(content[2].strip()),\n            )\n    else:\n        # Extract first\n        pattern = r\"First_pixel \\(w.r.t. original_image\\):\" + SC_N_PATTERN\n        match = re.search(pattern, content)\n        first_pixel = int(match.group(1))\n        # Extract last\n        pattern = r\"Last_pixel \\(w.r.t. original_image\\):\" + SC_N_PATTERN\n        match = re.search(pattern, content)\n        last_pixel = int(match.group(1))\n        # difference\n        n_pixels = last_pixel - first_pixel + 1\n\n    # ----------------------------------------\n\n    # Fill the dictionary\n    datewise_metadata = {\n        \"sat_id\": sat_id,\n        \"orbit\": geometry,\n        \"acq_date\": acq_date,\n        \"azimuth0time\": azimuth0time,\n        \"range0time\": range0time,\n        \"prf\": prf,\n        \"rsr\": rsr,\n        \"wavelength\": wavelength,\n        \"orbit_fit\": orbfit,\n        \"range_spacing\": range_spacing,\n        \"azimuth_spacing\": azimuth_spacing,\n        \"center_lon\": center_lon,\n        \"center_lat\": center_lat,\n        \"center_h\": center_h,\n        \"n_azimuth\": n_azimuth,\n        \"n_range\": n_range,\n        \"1stAzimuth\": first_line,\n        \"1stRange\": first_pixel,\n        \"swath\": swath,\n        \"center_azimuth\": center_azimuth,\n        \"beta0\": beta0,\n        \"azimuth_resolution\": azimuth_resolution,\n        \"range_resolution\": range_resolution,\n        \"slant_range_resolution\": sr_resolutions,\n        \"nBursts\": 1,\n        \"burstInfo\": burst_n,\n        \"steering_rate\": steering_rate,\n        \"pri\": pri,\n        \"rank\": rank,\n        \"chirp_rate\": chirp_rate,\n        \"n_lines\": n_lines,\n        \"n_pixels\": n_pixels,\n        # -------------------------------------------------------------------\n    }\n\n    return datewise_metadata\n</code></pre>"},{"location":"api_reference/#slc-methods","title":"slc methods:","text":""},{"location":"api_reference/#depsi.slc","title":"<code>depsi.slc</code>","text":"<p>slc.py: Functions for SLC related operations.</p>"},{"location":"api_reference/#depsi.slc.ifg_to_slc","title":"<code>ifg_to_slc(mother_slc, ifgs)</code>","text":"<p>Convert a stack of interferograms to SLCs.</p> <p>The conversion will be implemented by conjugated multiplication of the interferograms complex values with the complex values of the mother SLC, and then dividing by the squared magnitude of the mother complex.</p> <p>Parameters:</p> Name Type Description Default <code>mother_slc</code> <code>Dataset</code> <p>Mother SLC. This Dataset should have three dimensions ('azimuth', 'range', 'time'). The 'azimuth' and 'range' dimensions should be the same as <code>ifgs</code>. The 'time' dimension should have size 1.</p> required <code>ifgs</code> <code>Dataset</code> <p>Interferograms. This Dataset should have three dimensions ('azimuth', 'range', 'time'). The 'azimuth' and 'range' dimensions should be the same as <code>mother_slc</code>.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>SLCS converted from the interferograms.</p> Source code in <code>depsi/slc.py</code> <pre><code>def ifg_to_slc(mother_slc, ifgs):\n    \"\"\"Convert a stack of interferograms to SLCs.\n\n    The conversion will be implemented by conjugated multiplication of the interferograms complex values\n    with the complex values of the mother SLC, and then dividing by the squared magnitude of the mother complex.\n\n    Parameters\n    ----------\n    mother_slc : Xarray.Dataset\n        Mother SLC. This Dataset should have three dimensions ('azimuth', 'range', 'time').\n        The 'azimuth' and 'range' dimensions should be the same as `ifgs`.\n        The 'time' dimension should have size 1.\n    ifgs : Xarray.Dataset\n        Interferograms. This Dataset should have three dimensions ('azimuth', 'range', 'time').\n        The 'azimuth' and 'range' dimensions should be the same as `mother_slc`.\n\n    Returns\n    -------\n    Xarray.Dataset\n        SLCS converted from the interferograms.\n    \"\"\"\n    slc_out = ifgs.copy()\n    meta_arr = np.array((), dtype=np.complex64)\n    slc_complex = da.apply_gufunc(\n        _slc_complex_recontruct, \"(),()-&gt;()\", mother_slc[\"complex\"], slc_out[\"complex\"], meta=meta_arr\n    )\n    slc_out = slc_out.assign({\"complex\": ((\"azimuth\", \"range\", \"time\"), slc_complex)})\n    return slc_out\n</code></pre>"},{"location":"api_reference/#utility-functions","title":"Utility Functions:","text":""},{"location":"api_reference/#depsi.utils","title":"<code>depsi.utils</code>","text":""},{"location":"dev_guide/","title":"Developer Guide","text":""},{"location":"dev_guide/#installation-guide","title":"Installation guide","text":"<p>The Python implementation of DePSI is under development. At present you can only install it from the GitHub repository.</p> <p>It is assumed that you have <code>mamba</code> installed. If not, you can find the installation instructions here. Other package managers like <code>conda</code> or <code>venv</code> can be used as well.</p> <p>Clone this repository and <code>cd</code> into it:</p> <pre><code>git clone git@github.com:TUDelftGeodesy/DePSI.git\ncd DePSI\n</code></pre> <p>Create a new conda environment (here we give an example name <code>depsi-dev</code>) with <code>mamba</code>.:</p> <pre><code>mamba create -c conda-forge -n depsi-dev python=3.12\n</code></pre> <p>Here we use Python 3.12 since we aim to support python 3.10 and above.</p> <p>Activate the environment:</p> <pre><code>mamba activate depsi-dev\n</code></pre> <p>Install this package in development mode, with extra dependencies for development and documentation:</p> <pre><code>pip install -e \".[dev,docs]\"\n</code></pre> <p>In the end, install the pre-commit hooks, which will run the checks before each commit: <pre><code>pre-commit install\n</code></pre></p>"},{"location":"dev_guide/#linting-and-formatting","title":"Linting and formatting","text":"<p>We use <code>ruff</code> for linting and formatting. If the pre-commit hooks are installed, the checks will be run automatically before each commit.</p> <p>To manually run the checks, use the following command in the root directory of the repository:</p> <pre><code>ruff check .\n</code></pre>"},{"location":"dev_guide/#testing","title":"Testing","text":"<p>We use <code>pytest</code> for testing. All tests are located in the <code>tests</code> directory.</p> <p>To run the tests, use the following command in the root directory of the repository:</p> <pre><code>pytest tests\n</code></pre> <p>The GitHub Actions will run the tests automatically for each push and pull-request on the <code>main</code> branch.</p>"},{"location":"dev_guide/#documentation","title":"Documentation","text":"<p>We use <code>mkdocs</code> for documentation. </p> <p>To check the documentation at local, use the following command in the root directory of the repository:</p> <pre><code>mkdocs serve\n</code></pre> <p>This will build and render the documentation at a local server. Follow the link provided in the terminal to view the documentation in the browser.</p>"},{"location":"usages/slc/","title":"SLC related methods","text":""},{"location":"usages/slc/#converting-coregistered-interferogram-stack-to-slc-stack","title":"Converting coregistered interferogram stack to SLC stack","text":"<p>After reading the coregistered interferogram stack and the mother SLC with <code>sarxarray</code>, the SLC stack can be reconstructed using the <code>ifg_to_slc</code> method. The method takes the mother SLC and the coregistered interferogram stack as input and returns the reconstructed SLC stack.</p> <pre><code>import sarxarray\nfrom depsi.slc import ifg_to_slc\nfrom pathlib import Path\n\nf_mother_slc = 'path/to/mother_slc.raw' # Path to the mother SLC binary file\nf_ifgs = list(sorted(Path('dir_ifgs').rglob(\"2*/ifnteferogram.raw\")))  # List of paths of coregistered interferograms\nshape = (10768, 40588) # Shape of the stack, (nrows, ncols)\nreading_chunks = (2000, 2000)  # Reading chunks for lazy loading, (nrows, ncols)\n\n# Lazy loading mother SLC and ifg stack\nmother = sarxarray.from_binary([f_mother_slc], shape, dtype=np.complex64, chunks=reading_chunks)\nifgs = sarxarray.from_binary(f_ifgs, shape, dtype=np.complex64, chunks=reading_chunks)\n\n# Generate reconstructed SLCs\nslc_recon = ifg_to_slc(mother, ifgs)\n</code></pre>"}]}